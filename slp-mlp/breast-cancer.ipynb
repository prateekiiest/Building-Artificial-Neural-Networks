{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "4498ebbb-beac-4a85-a2dc-533d664588ce",
        "_uuid": "017b79012bfea8cdeecd008194da893189bdc264"
      },
      "cell_type": "markdown",
      "source": "# Building SLP and MLP for Breast Cancer Data"
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "import tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler, Normalizer\nfrom sklearn.decomposition import PCA as sklearnPCA\n\n# Supress unnecessary warnings so that presentation looks clean\nimport warnings\nwarnings.filterwarnings(\"ignore\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "62d537aa-38c0-4eb7-a8e3-b1306b8bf741",
        "_uuid": "9388f2b05168c6dde3c3ae8c947a3ffcbf1acf9a"
      },
      "cell_type": "markdown",
      "source": "---\n\n# 4. Explore Dataset\n## 4-1) Import dataset"
    },
    {
      "metadata": {
        "_cell_guid": "3f44bd61-891a-4907-9ad4-4b24e2a476de",
        "_uuid": "226c6831972f96b5d8b50b66a6ff517c175b3e3b",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "wbcd = pd.read_csv(\"../input/breast/breast-cancer-wisconsin.csv\")\nwbcd.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "05f38efc1164eb33724d102575f60c0a1b5446d6",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "wbcd = wbcd.replace('?', 0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "4478f2f7-c82d-4b1d-8cc3-3a3d300c66f1",
        "_uuid": "5904e176a7d70ca730bafc6eacbe2d8e4ab693ac",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "print(\"This WBCD dataset is consisted of\",wbcd.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "aa40b614-5d70-492e-a672-de808a12dcde",
        "_uuid": "4b3285686b3d66efcf5d5441be5ebf4eeb695c3b"
      },
      "cell_type": "markdown",
      "source": "## 4-3) Summary the Diagnosis"
    },
    {
      "metadata": {
        "_cell_guid": "52ad3062-d000-49c5-94a7-4ad80c3a25de",
        "_uuid": "a8d564395cfb9b4b6c2758d37546a2d4074906f5",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "sns.countplot(wbcd['class'],label=\"Count\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "b5460d21-3fc8-4e6d-ba23-acf4b0fdb354",
        "_uuid": "99491c255fb20c56cf862fb91fbe39830b9cff49"
      },
      "cell_type": "markdown",
      "source": "## 4-4) Correlation Plot of 30 features\nexcept **id, diagnosis** columns => wbcd.iloc[:,2:]"
    },
    {
      "metadata": {
        "_cell_guid": "1d89200b-ab00-4a5d-b94b-9fac3ac11fc6",
        "_uuid": "5e640fffd0e3e9ade799a75d3ac417d4a894916e",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "corr = wbcd.iloc[:,:].corr()\ncolormap = sns.diverging_palette(420, 10, as_cmap = True)\nplt.figure(figsize=(8,8))\nsns.heatmap(corr, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 8},\n            cmap = colormap, linewidths=0.1, linecolor='white')\nplt.title('Correlation of Breast Cancer Data Features', y=1.05, size=15)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "fb7abe15-91ec-455c-9f06-488310ae10ef",
        "_uuid": "fd8c5caefa7ef7df8c9cfbe7c2fe5cfc047e1290"
      },
      "cell_type": "markdown",
      "source": "---\n\n# 5. Preparing Data for machine learning\n## 5-1) Divide \"Breast Cancer data\" into Train(70%) / Test data(30%)\nDivide the data into two(train/test) to see the predictive power of the model.\n"
    },
    {
      "metadata": {
        "_cell_guid": "010c6214-6da0-4a5c-9d0f-f5bf6a992790",
        "_uuid": "c7aa69db4e553eab98f8be59cbf1731746164b94",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "train,test = train_test_split(wbcd, test_size=0.3, random_state=42)\nprint(\"Training Data :\",train.shape)\nprint(\"Testing Data :\",test.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "6c4466ae-c775-4eaa-99e2-ae81a6f55f89",
        "_uuid": "d6d0f79538cf52db3322f311242d85447d7e0646",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "train_data = train\ntest_data = test\n\n\nprint(\"Training Data :\",train_data.shape)\nprint(\"Testing Data :\",test_data.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "372a53bd49a80dd43b357bdac077eff003f1122c",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "train.iloc[:,0:-1].head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "961687726398f35f502485cbb0cb55a87738396a",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "train_x = train_data.iloc[:,0:-1]\ntrain_x = MinMaxScaler().fit_transform(train_x)\nprint(\"Training Data :\", train_x.shape)\n\n# Testing Data\ntest_x = test_data.iloc[:,0:-1]\ntest_x = MinMaxScaler().fit_transform(test_x)\nprint(\"Testing Data :\", test_x.shape)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2488ad9c8b12b4042e9c50fef0698fe6408a2170",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "train_y = train_data.iloc[:,-1:]\ntrain_y[train_y== 2] = 0\ntrain_y[train_y== 4 ] = 1\nprint(\"Training Data :\", train_y.shape)\n\n# Testing Data\ntest_y = test_data.iloc[:,-1:]\ntest_y[test_y== 2] = 0\ntest_y[test_y==4 ] = 1\nprint(\"Testing Data :\", test_y.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "fa3ff52a-9d01-46af-ba41-945d6f960f3b",
        "_uuid": "997e336dc4501dc0dcb5db2c1f7fcac363b381f3"
      },
      "cell_type": "markdown",
      "source": "---\n\n# 6. Make ANN-SLP Model\n## 6-1) Make \"Placeholder\" for dinamic variable allocation\nPlaceholder is one of the function in tensorflow.\nIt is a space to put and change values while the program is running.\n* for X, a place must have 30 columns, since wbcd data has 30 features.\n* for Y, a place must have 1 columns, since the results has 1 outcome.\n* If you see the row \"None\", it means it has no size limits. (You can write -1 instead of \"None\")"
    },
    {
      "metadata": {
        "_cell_guid": "b2f628ba-b54f-4606-88b3-a94e8d04abfc",
        "_uuid": "203de4b9aea972e31c4887e81b69011e978b4ae2",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "X = tf.placeholder(tf.float32, [None,9])\nY = tf.placeholder(tf.float32, [None, 1])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "8b4db30e-6d9a-4218-a13c-77646f448e06",
        "_uuid": "6150a33f862340ace07c778141028d9e7ef43c9a"
      },
      "cell_type": "markdown",
      "source": "## 6-2) Make Weight, Bias value with randomly\n* W(weight) : why **[30,1]**?  16 for 16 features, 1 for 1 Outcome(results).\n* P(weight): why **[10,1]**? 10 for 10 PCA features, 1 for 1 Outcome(results).\n* b(bias) : why **[1]**?  outcome has 1 layers."
    },
    {
      "metadata": {
        "_cell_guid": "d1f84757-a576-4e5b-8906-bce6dba3c84c",
        "_uuid": "e8791b6931c8197198f85a99f20e777697774906",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# weight\nW = tf.Variable(tf.random_normal([9,1], seed=0), name='weight')\n\n# bias\nb = tf.Variable(tf.random_normal([1], seed=0), name='bias')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "97f607ce-213d-4e4a-a8e9-cc30c47d68a1",
        "_uuid": "a16646d548cc2bf15303c815a12627e326fb7d7c"
      },
      "cell_type": "markdown",
      "source": "## 6-3) Make Output Results\n * **Output = Weight * Input + Bias**\n * tf.matmul() : for array multiply"
    },
    {
      "metadata": {
        "_cell_guid": "56416585-56a4-442d-88f8-6cb2fb9f9101",
        "_uuid": "0fe1de012691c4da71e4b5dc9324204bd61cebae",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "logits = tf.matmul(X,W) + b",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "6d85b3f5-d900-4889-92f3-23cb539f6ed9",
        "_uuid": "eb674379b6d9312560f7b8bbc9d87da457f3d181"
      },
      "cell_type": "markdown",
      "source": ""
    },
    {
      "metadata": {
        "_cell_guid": "4881aae2-320c-4af2-a243-41c11a8f0c34",
        "_uuid": "9f792f4ee5c1fdc31de05ca8d4bb68c600bba200",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "hypothesis = tf.nn.sigmoid(logits)\n\ncost_i = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,labels=Y)\ncost = tf.reduce_mean(cost_i)\n# cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "980d8382-b7b7-44ad-8b0e-cd13b2d2f149",
        "_uuid": "c2965cd1421afd8e83fc8596a34298e7be5c7e15"
      },
      "cell_type": "markdown",
      "source": ""
    },
    {
      "metadata": {
        "_cell_guid": "aadf51d1-b5dc-49f5-991c-45ca6ae0ecb2",
        "_uuid": "eebe65b3e3633ec738b05288d549ec9e4d13014c",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "940eb4ef-6f9e-41ed-b692-c4d1d244dfec",
        "_uuid": "df417da0731dabafabe0119275f83d7cb1126641"
      },
      "cell_type": "markdown",
      "source": "## 6-6) Compare : original vs. prediction"
    },
    {
      "metadata": {
        "_cell_guid": "4984c760-fb34-45cf-82d6-6862438ab466",
        "_uuid": "26d175949daff82bd0a08115dd8600b1976f795e",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "prediction = tf.cast(hypothesis > 0.5, dtype=tf.float32)\ncorrect_prediction = tf.equal(prediction, Y)\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, dtype=tf.float32))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "84fdb2ef-75a6-462c-8688-146e546554a9",
        "_uuid": "ebec32f59831eb056d33d9231d3310122e4a82bb"
      },
      "cell_type": "markdown",
      "source": "## 6-7) Activate Model"
    },
    {
      "metadata": {
        "_cell_guid": "b49eb9b5-3381-4fae-a042-d0ca1d722e3a",
        "_uuid": "05ecab0b5cc0c38ec015c2e5d66f9901b71ca463",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "with tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for step in range(10001):\n        sess.run(train, feed_dict={X: train_x, Y: train_y})\n        if step % 1000 == 0:\n            loss, acc = sess.run([cost, accuracy], feed_dict={X: train_x, Y: train_y})\n            print(\"Step: {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}\".format(step, loss, acc))\n            \n    train_acc = sess.run(accuracy, feed_dict={X: train_x, Y: train_y})\n    test_acc,test_predict,test_correct = sess.run([accuracy,prediction,correct_prediction], feed_dict={X: test_x, Y: test_y})\n    print(\"Model Prediction =\", train_acc)\n    print(\"Test Prediction =\", test_acc)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "730a62f0-ff6f-4c85-b8d3-d2d75c98595f",
        "_uuid": "7cfedba0c43751afcc1c3b783307ea3a3e1d7734"
      },
      "cell_type": "markdown",
      "source": "---\n\n# 7. ANN Model Summary & Compare\n## 7-1) ANN - SLP Model\n* train_x, test_x : normalization data\n* 9 features\n* train_y, test_y"
    },
    {
      "metadata": {
        "_cell_guid": "64fe8680-42d7-463e-a6e9-12a9bb0cb48e",
        "_uuid": "bc07298da8f7fca38547a9ac23a7854e80fe390e",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "def ann_slp():\n    print(\"===========Data Summary===========\")\n    print(\"Training Data :\", train_x.shape)\n    print(\"Testing Data :\", test_x.shape)\n\n    X = tf.placeholder(tf.float32, [None,9])\n    Y = tf.placeholder(tf.float32, [None, 1])\n\n    W = tf.Variable(tf.random_normal([9,1], seed=0), name='weight')\n    b = tf.Variable(tf.random_normal([1], seed=0), name='bias')\n\n    logits = tf.matmul(X,W) + b\n    hypothesis = tf.nn.sigmoid(logits)\n    \n    cost_i = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,labels=Y)\n    cost = tf.reduce_mean(cost_i)\n\n    train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n\n    prediction = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n    correct_prediction = tf.equal(prediction, Y)\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, dtype=tf.float32))\n\n    \n    loss_ans= []\n    acc_ans = []\n    print(\"\\n============Processing============\")\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        for step in range(10001):\n            sess.run(train, feed_dict={X: train_x, Y: train_y})\n            if step % 1000 == 0:\n                loss, acc = sess.run([cost, accuracy], feed_dict={X: train_x, Y: train_y})\n                loss_ans.append(loss)\n                acc_ans.append(acc)\n                print(\"Step: {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}\".format(step, loss, acc))\n\n        train_acc = sess.run(accuracy, feed_dict={X: train_x, Y: train_y})\n        test_acc,test_predict,test_correct = sess.run([accuracy,prediction,correct_prediction], feed_dict={X: test_x, Y: test_y})\n        \n        print(\"\\n============Results============\")\n        print(\"Model Prediction =\", train_acc)\n        print(\"Test Prediction =\", test_acc)\n        \n        return train_acc,test_acc, loss_ans, acc_ans\n    \nann_slp_train_acc, ann_slp_test_acc, loss_ans, acc_ans = ann_slp()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a05292bece92ad85bf6c99a0171deb27d9eab9e6",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#import seaborn as sn\nsns.pointplot([i for i in range(len(loss_ans))], loss_ans)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "24c5639d-77ba-4a22-add1-a9fa75310d69",
        "_uuid": "f532469f3c320cf774d41972d177bb3d52219b80"
      },
      "cell_type": "markdown",
      "source": "\n## 7-3) ANN - MLP Model\n* train_x, test_x : normalization data\n* 9 features\n* train_y, test_y"
    },
    {
      "metadata": {
        "_cell_guid": "3fde2a00-71cc-401e-81d5-102916b200f4",
        "_uuid": "11280c554427605e611ef9880fef6dc68306e587",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "def ann_mlp():\n    print(\"===========Data Summary===========\")\n    print(\"Training Data :\", train_x.shape)\n    print(\"Testing Data :\", test_x.shape)\n\n    X = tf.placeholder(tf.float32, [None,9])\n    Y = tf.placeholder(tf.float32, [None, 1])\n\n    # input\n    W1 = tf.Variable(tf.random_normal([9,18], seed=0), name='weight1')\n    b1 = tf.Variable(tf.random_normal([18], seed=0), name='bias1')\n    layer1 = tf.nn.sigmoid(tf.matmul(X,W1) + b1)\n\n    # hidden1\n    W2 = tf.Variable(tf.random_normal([18,18], seed=0), name='weight2')\n    b2 = tf.Variable(tf.random_normal([18], seed=0), name='bias2')\n    layer2 = tf.nn.sigmoid(tf.matmul(layer1,W2) + b2)\n\n    # hidden2\n    W3 = tf.Variable(tf.random_normal([18,27], seed=0), name='weight3')\n    b3 = tf.Variable(tf.random_normal([27], seed=0), name='bias3')\n    layer3 = tf.nn.sigmoid(tf.matmul(layer2,W3) + b3)\n\n    # output\n    W4 = tf.Variable(tf.random_normal([27,1], seed=0), name='weight4')\n    b4 = tf.Variable(tf.random_normal([1], seed=0), name='bias4')\n    logits = tf.matmul(layer3,W4) + b4\n    hypothesis = tf.nn.sigmoid(logits)\n\n    cost_i = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,labels=Y)\n    cost = tf.reduce_mean(cost_i)\n\n    train = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(cost)\n\n    prediction = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n    correct_prediction = tf.equal(prediction, Y)\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, dtype=tf.float32))\n\n    loss_ans = []\n    print(\"\\n============Processing============\")\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        for step in range(10001):\n            sess.run(train, feed_dict={X: train_x, Y: train_y})\n            if step % 1000 == 0:\n                loss, acc = sess.run([cost, accuracy], feed_dict={X: train_x, Y: train_y})\n                \n                print(\"Step: {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}\".format(step, loss, acc))\n            loss_ans.append(loss)\n\n        train_acc = sess.run(accuracy, feed_dict={X: train_x, Y: train_y})\n        test_acc,test_predict,test_correct = sess.run([accuracy,prediction,correct_prediction], feed_dict={X: test_x, Y: test_y})\n        \n        print(\"\\n============Results============\")\n        print(\"Model Prediction =\", train_acc)\n        print(\"Test Prediction =\", test_acc)\n        \n        return train_acc,test_acc, loss_ans\n    \nann_mlp_train_acc, ann_mlp_test_acc , loss_ans = ann_mlp()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "03eab38201ece1c39aef713ddb336059a073a392",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "sns.pointplot([i for i in range(len(loss_ans))], loss_ans)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "f28941c22de9fb30c6d666c816550897798a9d04"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
