{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler, Normalizer\nfrom sklearn.decomposition import PCA as sklearnPCA\n\n# Supress unnecessary warnings so that presentation looks clean\nimport warnings\nwarnings.filterwarnings(\"ignore\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "wbcd = pd.read_csv(\"../input/voteshouse/house-votes-84.csv\")\nwbcd.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7332dc14cdba1ee63e8ffb48fbad1dd43c271e35"
      },
      "cell_type": "code",
      "source": "print(\"This WBCD dataset is consisted of\",wbcd.shape)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3628dd6d9d2ea923b0a672b7d08a223af9c58ba9"
      },
      "cell_type": "code",
      "source": "sns.countplot(wbcd['party'],label=\"Count\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6e7f1cda34fedf7ae5ed5d8c028ca3ede78b573b"
      },
      "cell_type": "code",
      "source": "corr = wbcd.iloc[:,:].corr()\ncolormap = sns.diverging_palette(220, 10, as_cmap = True)\nplt.figure(figsize=(8,8))\nsns.heatmap(corr, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 8},\n            cmap = colormap, linewidths=0.1, linecolor='white')\nplt.title('Correlation of House Votes Features', y=1.05, size=15)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2f12d5fd768a302dd19e5051067a97c610e1cbb8"
      },
      "cell_type": "code",
      "source": "train,test = train_test_split(wbcd, test_size=0.3, random_state=42)\nprint(\"Training Data :\",train.shape)\nprint(\"Testing Data :\",test.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1baab6c6683f9a840313b521d0250cba525dd6c3"
      },
      "cell_type": "code",
      "source": "train_data = train\ntest_data = test\n\n\nprint(\"Training Data :\",train_data.shape)\nprint(\"Testing Data :\",test_data.shape)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f66f59ff2cc8ac869d5256d347456fa081afa9dd"
      },
      "cell_type": "code",
      "source": "train_x = train_data.iloc[:,0:-1]\ntrain_x = MinMaxScaler().fit_transform(train_x)\nprint(\"Training Data :\", train_x.shape)\n\n# Testing Data\ntest_x = test_data.iloc[:,0:-1]\ntest_x = MinMaxScaler().fit_transform(test_x)\nprint(\"Testing Data :\", test_x.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "166d03ea40d8b22b2a4cd2a819ca7fb93b8c8fbd"
      },
      "cell_type": "code",
      "source": "train_y = train_data.iloc[:,-1:]\ntrain_y[train_y == 'republican'] = 0\ntrain_y[train_y == 'democrat' ] = 1\nprint(\"Training Data :\", train_y.shape)\n\n# Testing Data\ntest_y = test_data.iloc[:,-1:]\ntest_y[test_y == 'republican'] = 0\ntest_y[test_y == 'democrat' ] = 1\nprint(\"Testing Data :\", test_y.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "27ec5bca8b3df78e763ce6d0bc1c191d560c7ff1"
      },
      "cell_type": "code",
      "source": "X = tf.placeholder(tf.float32, [None,16])\nY = tf.placeholder(tf.float32, [None, 1])\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5472d92af5833b6ef054a1d5581cad5607ad4cda"
      },
      "cell_type": "code",
      "source": "# weight\nW = tf.Variable(tf.random_normal([16,1], seed=0), name='weight')\n\n# bias\nb = tf.Variable(tf.random_normal([1], seed=0), name='bias')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "094033513f5060cae0b96200d5b63446ac880999"
      },
      "cell_type": "code",
      "source": "logits = tf.matmul(X,W) + b",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "abe38643917bb00b75c327fb05fc2923723c1be6"
      },
      "cell_type": "code",
      "source": "hypothesis = tf.nn.sigmoid(logits)\n\ncost_i = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,labels=Y)\ncost = tf.reduce_mean(cost_i)\n# cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7beaa766ca55c1a7c80e40f5d556ddb7e9ce08fa"
      },
      "cell_type": "code",
      "source": "hypothesis = tf.nn.sigmoid(logits)\n\ncost_i = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,labels=Y)\ncost = tf.reduce_mean(cost_i)\n# cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6b15bc19d5b8d44c31647052d54a7ee10a5ac936"
      },
      "cell_type": "code",
      "source": "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5cd59da429dfe3925ba982942a2aad4f16c550ba"
      },
      "cell_type": "code",
      "source": "prediction = tf.cast(hypothesis > 0.5, dtype=tf.float32)\ncorrect_prediction = tf.equal(prediction, Y)\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, dtype=tf.float32))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4e3870098718268d7bd45bfc258070e10e14c571"
      },
      "cell_type": "code",
      "source": "with tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for step in range(10001):\n        sess.run(train, feed_dict={X: train_x, Y: train_y})\n        if step % 1000 == 0:\n            loss, acc = sess.run([cost, accuracy], feed_dict={X: train_x, Y: train_y})\n            print(\"Step: {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}\".format(step, loss, acc))\n            \n    train_acc = sess.run(accuracy, feed_dict={X: train_x, Y: train_y})\n    test_acc,test_predict,test_correct = sess.run([accuracy,prediction,correct_prediction], feed_dict={X: test_x, Y: test_y})\n    print(\"Model Prediction =\", train_acc)\n    print(\"Test Prediction =\", test_acc)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e90fc69a2c5d0568e6a86fffdaa9520a7c4383e2"
      },
      "cell_type": "code",
      "source": "def ann_slp():\n    print(\"===========Data Summary===========\")\n    print(\"Training Data :\", train_x.shape)\n    print(\"Testing Data :\", test_x.shape)\n\n    X = tf.placeholder(tf.float32, [None,16])\n    Y = tf.placeholder(tf.float32, [None, 1])\n\n    W = tf.Variable(tf.random_normal([16,1], seed=0), name='weight')\n    b = tf.Variable(tf.random_normal([1], seed=0), name='bias')\n\n    logits = tf.matmul(X,W) + b\n    hypothesis = tf.nn.sigmoid(logits)\n    \n    cost_i = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,labels=Y)\n    cost = tf.reduce_mean(cost_i)\n\n    train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n\n    prediction = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n    correct_prediction = tf.equal(prediction, Y)\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, dtype=tf.float32))\n\n    \n    loss_ans= []\n    acc_ans = []\n    print(\"\\n============Processing============\")\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        for step in range(10001):\n            sess.run(train, feed_dict={X: train_x, Y: train_y})\n            if step % 1000 == 0:\n                loss, acc = sess.run([cost, accuracy], feed_dict={X: train_x, Y: train_y})\n                loss_ans.append(loss)\n                acc_ans.append(acc)\n                print(\"Step: {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}\".format(step, loss, acc))\n\n        train_acc = sess.run(accuracy, feed_dict={X: train_x, Y: train_y})\n        test_acc,test_predict,test_correct = sess.run([accuracy,prediction,correct_prediction], feed_dict={X: test_x, Y: test_y})\n        \n        print(\"\\n============Results============\")\n        print(\"Model Prediction =\", train_acc)\n        print(\"Test Prediction =\", test_acc)\n        \n        return train_acc,test_acc, loss_ans, acc_ans\n    \nann_slp_train_acc, ann_slp_test_acc, loss_ans, acc_ans = ann_slp()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9c1d427109000e620c775fa0ec4c9cc28fd8c111"
      },
      "cell_type": "markdown",
      "source": "## MLP"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f02063d52ffbdb8064e26d9e3869edfd86a219e4"
      },
      "cell_type": "code",
      "source": "def ann_mlp():\n    print(\"===========Data Summary===========\")\n    print(\"Training Data :\", train_x.shape)\n    print(\"Testing Data :\", test_x.shape)\n\n    X = tf.placeholder(tf.float32, [None,16])\n    Y = tf.placeholder(tf.float32, [None, 1])\n\n    # input\n    W1 = tf.Variable(tf.random_normal([16,32], seed=0), name='weight1')\n    b1 = tf.Variable(tf.random_normal([32], seed=0), name='bias1')\n    layer1 = tf.nn.sigmoid(tf.matmul(X,W1) + b1)\n\n    # hidden1\n    W2 = tf.Variable(tf.random_normal([32,48], seed=0), name='weight2')\n    b2 = tf.Variable(tf.random_normal([48], seed=0), name='bias2')\n    layer2 = tf.nn.sigmoid(tf.matmul(layer1,W2) + b2)\n\n    # hidden2\n    W3 = tf.Variable(tf.random_normal([48,64], seed=0), name='weight3')\n    b3 = tf.Variable(tf.random_normal([64], seed=0), name='bias3')\n    layer3 = tf.nn.sigmoid(tf.matmul(layer2,W3) + b3)\n    \n     # hidden3\n    W4 = tf.Variable(tf.random_normal([48,64], seed=0), name='weight4')\n    b4 = tf.Variable(tf.random_normal([64], seed=0), name='bias4')\n    #layer4 = tf.nn.sigmoid(tf.matmul(layer3,W4) + b4)\n\n    # output\n    W5 = tf.Variable(tf.random_normal([64,1], seed=0), name='weight5')\n    b5 = tf.Variable(tf.random_normal([1], seed=0), name='bias5')\n    logits = tf.matmul(layer3,W5) + b5\n    hypothesis = tf.nn.sigmoid(logits)\n\n    cost_i = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,labels=Y)\n    cost = tf.reduce_mean(cost_i)\n\n    train = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(cost)\n\n    prediction = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n    correct_prediction = tf.equal(prediction, Y)\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, dtype=tf.float32))\n\n    loss_ans = []\n    print(\"\\n============Processing============\")\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        for step in range(10001):\n            sess.run(train, feed_dict={X: train_x, Y: train_y})\n            if step % 1000 == 0:\n                loss, acc = sess.run([cost, accuracy], feed_dict={X: train_x, Y: train_y})\n                loss_ans.append(loss)\n                print(\"Step: {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}\".format(step, loss, acc))\n            \n\n        train_acc = sess.run(accuracy, feed_dict={X: train_x, Y: train_y})\n        test_acc,test_predict,test_correct = sess.run([accuracy,prediction,correct_prediction], feed_dict={X: test_x, Y: test_y})\n        \n        print(\"\\n============Results============\")\n        print(\"Model Prediction =\", train_acc)\n        print(\"Test Prediction =\", test_acc)\n        \n        return train_acc,test_acc, loss_ans\n    \nann_mlp_train_acc, ann_mlp_test_acc , loss_ans = ann_mlp()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8fe261da309d2d51865ae7e5425dfee868899222"
      },
      "cell_type": "code",
      "source": "loss3 = loss_ans",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "2695bb465254f30a4f2dd522ef9c5e51ffd808fd"
      },
      "cell_type": "code",
      "source": "ax = sns.lineplot([i for i in range(len(loss1))], loss1 , label = \"1 Hidden Layers\")\nax = sns.lineplot([i for i in range(len(loss2))], loss2 , label = \"2 Hidden Layer\" )\nax = sns.lineplot([i for i in range(len(loss3))], loss3, label = \"3 Hidden Layers\")\nax.set(xlabel = \"Loss Count\", ylabel = \"Loss value\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7102800f44d88d9d99c340262fd887d42292d52d"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
